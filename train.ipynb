{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy OCR Training\n",
    "\n",
    "This repository contains the code for training Easy OCR model on custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/achyutkneupane/deep-text-recognition-benchmark.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd deep-text-recognition-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Drive\n",
    "Connect with the Google Drive to save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the required packages using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LMDB dataset\n",
    "\n",
    "To create the LMDB dataset, run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_lmdb_dataset.py train_data train_data/labels.txt train_lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_lmdb_dataset.py val_data val_data/labels.txt val_lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I used [TPS-ResNet-BiLSTM-Attn.pth](https://drive.google.com/file/d/1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9/view?usp=sharing) and placed it in `models` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "file_id=\"1ajONZOgiG9pEYsQ-eBmgkVbMDuHgPCaY\"\n",
    "output=\"TPS-ResNet-BiLSTM-Attn.pth\"\n",
    "\n",
    "gdown.download(id=file_id, output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d models ] || mkdir models\n",
    "!mv TPS-ResNet-BiLSTM-Attn.pth models/TPS-ResNet-BiLSTM-Attn.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now, we can train the model using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "dataset_root: train_lmdb\n",
      "opt.select_data: ['/']\n",
      "opt.batch_ratio: ['1.0']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    train_lmdb\t dataset: /\n",
      "sub-directory:\t/.\t num samples: 5500\n",
      "num total samples of /: 5500 x 1.0 (total_data_usage_ratio) = 5500\n",
      "num samples of / per batch: 8 x 1.0 (batch_ratio) = 8\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/achyutkneupane/python/kec/ocr/new-trainer/deep-text-recognition-benchmark/train.py\", line 358, in <module>\n",
      "    train(opt)\n",
      "  File \"/Users/achyutkneupane/python/kec/ocr/new-trainer/deep-text-recognition-benchmark/train.py\", line 49, in train\n",
      "    train_dataset = Batch_Balanced_Dataset(opt)\n",
      "  File \"/Users/achyutkneupane/python/kec/ocr/new-trainer/deep-text-recognition-benchmark/dataset.py\", line 69, in __init__\n",
      "    self.dataloader_iter_list.append(iter(_data_loader))\n",
      "  File \"/Users/achyutkneupane/python/kec/ocr/new-trainer/deep-text-recognition-benchmark/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 440, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"/Users/achyutkneupane/python/kec/ocr/new-trainer/deep-text-recognition-benchmark/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 388, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/Users/achyutkneupane/python/kec/ocr/new-trainer/deep-text-recognition-benchmark/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1038, in __init__\n",
      "    w.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n",
      "    reduction.dump(process_obj, fp)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: cannot pickle 'Environment' object\n"
     ]
    }
   ],
   "source": [
    "!python train.py --train_data train_lmdb --valid_data val_lmdb --select_data \"/\" --batch_ratio 1.0 --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --saved_model models/TPS-ResNet-BiLSTM-Attn.pth --batch_size 8 --data_filtering_off --workers 4 --batch_max_length 80 --num_iter 10 --valInterval 5 --FT --sensitive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
